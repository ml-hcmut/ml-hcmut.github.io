---
layout: post
title: Generative Adversarial Networks
categories: [deep learning, generative models]
---
Generative Adversarial Network (GAN), inspired by the game theory, was first introduced in 2014 [(Goodfellow, et.al.)](https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html). GAN is a likelihood-free framework to estimate a deep generative models, and have been applied to solve so many interesting real world problems such as super-resolution, text-to-image synthesize, image inpainting, etc. In this blog, we will investigate how GANs work and the theory behind them.

## **Why GANs?**
<hr>

## **Adversarial Network**
In this new framework for learning a generative models via an **adversarial** **training**, we will simutaneously train two models: the generative model that will capture the target distribution of data (then we can sample from) and the discriminative model that will assign a probability to determine whether a sample comes from the generator or the target distribution. In the other words, the generator is trained to fool the discriminator, and the discriminator need to correctly classify which are real samples or generated samples.
<hr>

## **Learning Objective**

In this part, we will derive the training objective function of generative adversarial networks and analyse the theories of adversarial nets. GANs contains two seperate components: the generative model and discriminative model, and these models can be parameterized by using neural networks. Let we denote $G_{\theta}$ is the generator and $D_{\phi}$ is the discriminator, where $\theta$ and $\phi$ are the set of parameters correspond to two seperate neural networks. Then, we denote $\mathbf{x} = \{x^{(1)}, x^{(2)}, x^{(3)},..., x^{(m)} \}$ is a batch of real samples drawn from the distribution $p_{data}$ and $\mathbf{z} = \{z^{(1)}, z^{(2)}, z^{(3)},..., z^{(m)} \}$ is a batch of noise samples drawn from the distribution $p_z$. In general, training GANs is to make the discriminator will assign probability of 1 for real samples and 0 for generated samples. Whereas, the generator tries to fool the discriminator that the discriminator will assign high probability for generated samples by the generator. First consider the binary crossentropy loss function

$$
\begin{align*}
    \mathcal{L}_{BCE}(y, \hat{y}) &= -\sum_{i=1}^D\;y_i\log{\hat{y_i}} + (1-y_i)\log(1-\hat{y_i})
\end{align*}
$$

where $y, \hat{y} \in R^D$. In the context of GANs, due to the objective of the discriminator that is to assign probability of 1 for $D(x)$, and 0 for  $D_{\phi}(G_{\theta}(z))$. So we can write this objective in the form of 

$$
\begin{equation}
    Objective(D_{\phi}) = min\; (\mathcal{L}_{BCE}(p(x), D_{\phi}(x)) +\mathcal{L}_{BCE}(p(G_{\theta}(z)), D_{\phi}(G_{\theta}(z))
\end{equation}
$$

Because the probability of real samples $p(x^{(i)})$ and generated sample $p(G_{\theta}(z^{(i)})$  are  1 and 0 respectively. Therefore we write down two loss function in equation (2) as

$$
\begin{align}
    \mathcal{L}_{BCE}(p(x), D_{\phi}(x)) &= -\sum_{i=1}^{D}\;p(x^{(i)})\log{D_{\phi}(x^{(i)})} + (1-p(x^{(i)}))\log(1-D_{\phi}(x^{(i)}))  \\
    & = -\sum_{i=1}^{D}\;p(x^{(i)})\log{D_{\phi}(x^{(i)})} \\
    & = -\sum_{i=1}^{D}\;\log{D_{\phi}(x^{(i)})} = - \mathbf{E}_{x \sim p_{data}}\left[\log{D_{\phi}(x)} \right]
\end{align}
$$

$$
\begin{align}
    \mathcal{L}_{BCE}(p(G_{\theta}(z)), D_{\phi}(G_{\theta}(z))) 
    & =-\sum_{i=1}^{D}\;p(G_{\theta}(z^{(i)}))\log{D_{\phi}(G_{\theta}(z^{(i)}))} + (1-p(G_{\theta}(z^{(i)})))\log(1-D_{\phi}(G_{\theta}(z^{(i)}))) \\
    & = -\sum_{i=1}^{D}\; (1-p(G_{\theta}(z^{(i)})))\log(1-D_{\phi}(G_{\theta}(z^{(i)})))\\
    &= -\sum_{i=1}^{D} \log(1-D_{\phi}(G_{\theta}(z^{(i)}))) \\
    & = - \mathbf{E}_{z \sim p_{z}}\left[{\log(1-D_{\phi}(G_{\theta}(z^{(i)})))} \right]
\end{align}
$$

From those equations above, the objective of discriminative model can be express as 

$$
\begin{align*}
    Objective(D_{\phi}) &= min\; \left( -\sum_{i=1}^{D}\;\log{D_{\phi}(x^{(i)})} + \log(1-D_{\phi}(G_{\theta}(z^{(i)}))) \right) \\
    & = max\; \left( \sum_{i=1}^{D}\;\log{D_{\phi}(x^{(i)})} + \log(1-D_{\phi}(G_{\theta}(z^{(i)}))) \right) \\
    & = max\; \left(\mathbf{E}_{x \sim p_{data}}\left[\log{D_{\phi}(x)} \right] + \mathbf{E}_{z \sim p_{z}}\left[{\log(1-D_{\phi}(G_{\theta}(z^{(i)})))} \right] \right)
\end{align*}
$$

Finally, the objective of the generative model is to generate samples that are much similar to the real samples that the discriminator cannot realize. Formally, training the generator is to decrease the chance that the discriminator will assign probability of 0 for $D_{\phi}(G_{\theta}(z)$ where $z$ is noisy sample randomly drawn from the prior distribution.

$$
\begin{align}
    Objective(G_{\theta}) 
        &= max\; \left(
        \mathcal{L}_{BCE}(p(G_{\theta}(z)), D_{\phi}(G_{\theta}(z))\right)
        \\
        &= max \; -\sum_{i=1}^{D} \log(1-D_{\phi}(G_{\theta}(z^{(i)}))) \\
        & = min \;\mathbf{E}_{z \sim p_{z}}\left[{\log(1-D_{\phi}(G_{\theta}(z^{(i)})))} \right]
\end{align}
$$

Wrap them up! The learning objective of GANs includes two different training processes of generator and discriminator with different objectives. From the derivations above, we can realize that, training GANs is a minimax game of two competitors $D_{\phi}$ and $G_{\theta}$, associated with a value function $V(D_{\phi}, G_{\theta})$. In that game, the discriminator $D_{\phi}$ wants to maximize certain quantity whereas, its opponent - the generator $G_{\theta}$ is trying to that quantity. Concretely,

$$
min_{G_{\theta}}max_{D_{\phi}}V(D_{\phi}, G_{\theta})
$$

<hr>

## **Training** 

<hr>

## **Sampling**

Written by [Quang Nguyen](https://quang-ngh.github.io)