---
layout: post
title: Generative Adversarial Networks
categories: [deep learning, generative models]
---
Generative Adversarial Network (GAN), inspired by the game theory, was first introduced in 2014 [(Goodfellow, et.al.)](https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html). GAN is a likelihood-free framework to estimate a deep generative models, and have been applied to solve so many interesting real world problems such as super-resolution, text-to-image synthesize, image inpainting, etc. In this blog, we will investigate how GANs work and the theory behind them.

## **Why GANs?**
<hr>

## **Adversarial Network**
In this new framework for learning a generative models via an **adversarial** **training**, we will simutaneously train two models: the generative model that will capture the target distribution of data (then we can sample from) and the discriminative model that will assign a probability to determine whether a sample comes from the generator or the target distribution. In the other words, the generator is trained to fool the discriminator, and the discriminator need to correctly classify which are real samples or generated samples.
<hr>

## **Learning Objective**

In this part, we will derive the training objective function of generative adversarial networks and analyse the theories of adversarial nets. GANs contains two seperate components: the generative model and discriminative model, and these models can be parameterized by neural networks. Let we denote $G_{\theta}$ is the generator and $D_{\phi}$ is the discriminator, where $\theta$ and $\phi$ are the set of parameters correspond to two neural networks. Then, we denote $\mathbf{x} = \{x^{(1)}, x^{(2)}, x^{(3)},..., x^{(m)} \}$ is a batch of real samples drawn from $p_{data}$ and $\mathbf{z} = \{z^{(1)}, z^{(2)}, z^{(3)},..., z^{(m)} \}$ is a batch of noise samples drawn from $p_z$. In general, training GANs is to make the discriminator assign probability of 1 for real samples and 0 for generated samples. Whereas, the generator tries to fool the discriminator that the discriminator will assign high probability for generated samples by the generator. First consider the binary crossentropy loss function

$$
\begin{equation}
    \mathcal{L}_{BCE}(y, \hat{y}) = -\sum_{i=1}^D\;y_i\log{\hat{y_i}} + (1-y_i)\log(1-\hat{y_i})
\end{equation}
$$

where $y, \hat{y} \in R^D$. In the context of GANs, due to the objective of the discriminator that is to assign probability of 1 for $D(x)$, and 0 for  $D_{\phi}(G_{\theta}(z))$. So we can write this objective in the form of 

$$
\begin{equation}
    Objective(D_{\phi}) = min\; (\mathcal{L}_{BCE}(p(x), D_{\phi}(x)) +\mathcal{L}_{BCE}(p(G_{\theta}(z)), D_{\phi}(G_{\theta}(z))
\end{equation}
$$

Because the probability of real samples $p(x^{(i)})$ and generated sample $p(G_{\theta}(z^{(i)})$  are  1 and 0 respectively. Therefore we write down two loss function in equation (2) as

$$
\begin{equation*}
    \mathcal{L}_{BCE}(u, D_{\phi}(x) = -\sum_{i=1}^{D}\;p(x^{(i)})\log{D(x^{(i)})} = -\sum_{i=1}^{D}\;\log{D(x^{(i)})}
\end{equation*}
$$

<hr>
## **Training** 

<hr>

## **Sampling**

Written by [Quang Nguyen](https://quang-ngh.github.io)